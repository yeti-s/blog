{"componentChunkName":"component---src-templates-md-contents-tsx","path":"/HDCorr/01_HDCorr/","result":{"data":{"markdownRemark":{"html":"<h1><a href=\"https://github.com/yeti-s/HDCorr\" target=\"_blank\" rel=\"nofollow\">HDCorr</a></h1>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">HD Map 제작을 위한 Point Cloud 측위 오차 보정 자동화</code></pre></div>\n<p>MMS(Mobile Mapping System) 장비를 이용하여 수집한 데이터를 센서 캘리브레이션을 통해 고정밀 도로 지도(HD Map)를 제작할 수 있다. 하지만 이 과정에서 동일한 공간을 촬영한 데이터에도 측위 오차가 발생하여 이를 보정해 줄 필요가 있다. HD Corr는 딥 러닝을 이용하여 Point Cloud의 측위 오차를 자동 보정하는 방법을 연구한다.\r\n<br>\r\n<br> 2D 이미지 데이터에서 Change Detection이라는 Task와 이를 위한 많은 모델이 존재한다.  이를 변형하여 3D Point Cloud에서 Same Area Detection이라는 Task를 수행한다. 생각처럼 잘 되진 않겠지만 이를 통해 Computer Vision 분야의 경험치를 쌓을 수 있다고 예상한다.</p>\n<h2>기존 보정 방식</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/0bd7f849064be23bff77ba9b3c9e1456/11930/0001_pcd_pos_error.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 24.479166666666668%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABWUlEQVR42j2My0obUQBA74Rk5mbmzjXWF74lPkhBCcHGSeOYmJgYjc6Ij9id0IWLgBSLpInBB1hEShctinv/9AizcHE4nM0Rpd071podCrVLCls/KTdv8U8fWPRamPEYtqOwLAvDMEgkEmitUa7GkkksaSOTToRKjSJtF5E76JMNumw2O9SDPpXGNeXDezKFADMRR6UGGRwaRrkujqPQegApTYQQETFDYBjio0W+9Yfq6R3h9w47Z7+otrrUWk8srx1jSpOR8QnGJqcZHh0j9WkIWylGppZIf/aYm80xl86RTnssLPjMzK8jipUrSvUexeolX8s/8Pw2pWqf5ewJlmOTVC5SaaSjIsfiBtn8GYff3miG/9k7emU/fCYIX9g7eEF88dvk/TarxXO8jQvKtRs2t+/JrARI5UTDpNKRbZ3CtCWZlZBSpUe98cjWzm8a4T+29/9S333iHShIm8n+8X9mAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/blog/static/0bd7f849064be23bff77ba9b3c9e1456/a59e9/0001_pcd_pos_error.webp 192w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/0ca9f/0001_pcd_pos_error.webp 384w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/dc9b9/0001_pcd_pos_error.webp 768w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/e2c2f/0001_pcd_pos_error.webp 1152w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/a742c/0001_pcd_pos_error.webp 1311w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/blog/static/0bd7f849064be23bff77ba9b3c9e1456/3b721/0001_pcd_pos_error.png 192w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/66595/0001_pcd_pos_error.png 384w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/fe486/0001_pcd_pos_error.png 768w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/d2d74/0001_pcd_pos_error.png 1152w,\n/blog/static/0bd7f849064be23bff77ba9b3c9e1456/11930/0001_pcd_pos_error.png 1311w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/blog/static/0bd7f849064be23bff77ba9b3c9e1456/fe486/0001_pcd_pos_error.png\"\n            alt=\"0001 pcd pos error\"\n            title=\"서로 다른 시간에 동일한 구간을 촬영한 Point Cloud\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\r\n동일한 공간을 서로 다른 시간에 촬영한 데이터를 보면 IMU 센서에 의한 오차로 인해 point들이 어느정도 차이가 나는 것을 볼 수 있다. 전체 HD Map을 제작하기 위해 이를 Translation과 Interpolation을 통해 데이터를 보정한다.</p>\n<h2>파이프라인</h2>\n<ul>\n<li>Feature Detection And Matching\n<ol>\n<li>Point Cloud의 Bird’s Eye View</li>\n<li>Feature Detecting</li>\n<li>Feature Matching</li>\n<li>Metric Learning</li>\n</ol>\n</li>\n<li>\n<ol>\n<li>Point cloud 샘플링</li>\n<li></li>\n</ol>\n</li>\n<li>Point cloud 샘플링\n<ul>\n<li>Road Marking Segmentation (Camera Image)</li>\n</ul>\n</li>\n<li>동일 도로 노면 감지\n<ul>\n<li>Metric Learning</li>\n<li>동일 부분 감지</li>\n</ul>\n</li>\n</ul>\n<h2>데이터 선택</h2>\n<p>도로 노면 정보를 이용하여 측위 오차 보정을 위해 높은 채널수의 라이다 센서를 필요로 한다. 또한 개인으로 진행하는 프로젝트이기 때문에 적당한 볼륨의 데이터를 선택해야 한다.</p>\n<h3><a href=\"https://apolloscape.auto/index.html\" target=\"_blank\" rel=\"nofollow\">Apolloscape</a></h3>\n<p>Apolloscape는 Lane Segmentation, Scene Parsing, Detection/Tracking 등 다양한 Task를 위한 데이터셋을 제공하고 있다. 특히 Lane Segmentation 데이터를 활용하여 카메라 이미지에서 Road Marking Segmentation 모델을 학습할 수 있고, 라이다 Projection을 통한 Point Cloud 샘플링을 진행할 수 있다고 예상하였다. 하지만 HD Map 제작을 위한 raw 데이터를 제공하지 않아 센서 캘리브레이션을 수행할 수 없었다.</p>\n<h3><a href=\"https://www.nuscenes.org/nuscenes\" target=\"_blank\" rel=\"nofollow\">Nuscene</a></h3>\n<p>기존 MMS 촬영 장비와 최대한 비슷한 환경을 가진 Nuscene 데이터를 채택했었다. 하지만 라이다의 채널과 카메라의 초당 프레임이 정밀한 지도를 만들기에 충분하지 않다고 판단하였다. 동일 사이트에 Nuplan 이라는 더 많은 센서를 이용해 촬영한 데이터가 존재하는데 개인 프로젝트로 진행하기에는 너무 큰 규모이기 때문에 고려하지 않았다.</p>\n<h3><a href=\"https://www.argoverse.org/av2.html\" target=\"_blank\" rel=\"nofollow\">Argoverse2</a></h3>\n<p>이미 데이터가 어느정도 정제되었지만 Rotation, Translation과 같은 캘리브레이션을 위한 정보도 함께 제공되어 적합하다고 판단하였다. 또한 더 많은 채널의 라이다와 더 높은 초당 카메라 이미지 수를 가지고 있어 Nuscene 보다 더 정밀한 지도를 만들 수 있다고 생각하여 위 데이터를 채택하였다.<br>\n<img src=\"/blog/927d438c9dc9a8a95407f55d7d6d1a1c/0000_nuscene_argoverse.png\" alt=\"\" title=\"각 데이터셋의 캘리브레이션 된 Potree\"></p>\n<h2>지리 공간 표시</h2>\n<p>IMU 정보를 가진 Ego Vehicle 데이터를 이용하여 GeoJSON 형식으로 촬영된 지리 공간을 웹에서 Openlayers API를 활용해 표시할 수 있도록 한다. Ego Vehicle의 Translation 값은 각 도시의 Original 좌표 기준 미터 단위의 Offset을 나타낸다.</p>\n<h2>데이터 구조</h2>\n<p>AV2 데이터셋은 아래 6개의 도시에서 수집되었다.</p>\n<ul>\n<li>Austin, Texas: 31 logs.</li>\n<li>Detroit, Michigan: 117 logs.</li>\n<li>Miami, Florida: 354 logs.</li>\n<li>Pittsburgh, Pennsylvania: 350 logs.</li>\n<li>Palo Alto, California: 22 logs.</li>\n<li>Washington, D.C.: 126 logs.</li>\n</ul>\n<p>AV2의 Sensor 데이터는 아래와 같은 폴더 구조를 가진다</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">|-- Root Directory\r\n    |\r\n    |-- [data id]\r\n    |   |-- annotations.feather\r\n    |   |-- city_SE3_egovehicle.feather\r\n    |   |-- calibration\r\n    |       |-- egovehicle_SE3_sensor.feather\r\n    |       |-- intrinsics.feather\r\n    |   |-- map\r\n    |       |-- [data id]___img_Sim2_city.json\r\n    |       |-- [data id]_ground_height_surface____[city name].npy\r\n    |       |-- log_map_archive_[data id]____[city name]_city_[city id].json\r\n    |   |-- sensors\r\n    |       |-- cameras\r\n    |           |-- ring_front_center\r\n    |               |-- [time_stamp].jpg\r\n    |               |-- ...\r\n    |           |-- ...\r\n    |       |-- lidar\r\n    |           |-- [time_stamp].feather\r\n    |           |-- ...\r\n    |   |-- potree (캘리브레이션을 통해 생성할 데이터)\r\n    |       |-- [data id].las\r\n    |       |-- hierarchy.bin\r\n    |       |-- log.txt\r\n    |       |-- metadata.json\r\n    |       |-- octree.bin\r\n    |-- ...</code></pre></div>\n<p>캘리브레이션 하여 생성한 Point Cloud 데이터를 .las 형식의 파일로 저장하고 <a href=\"https://github.com/potree/potree\" target=\"_blank\" rel=\"nofollow\">Potree</a>를 이용하여 이를 시각화 할 것이다.</p>\n<h2>센서 캘리브레이션</h2>\n<h3>라이다 캘리브레이션</h3>\n<p>AV2 데이터셋은 두 개의 32 Channel Lidar를 이용하여 하나의 64 Channel Lidar처럼 사용하였다. Up Lidar와 Down Lidar에 대한 Sensor Calibration을 제공하지만 Point Cloud는 이미 두 Lidar를 융합하여 Ego Vehicle의 좌표계로 변환해 둔 데이터를 제공하기에 쓸 일이 없다.</p>\n<h3>카메라 캘리브레이션</h3>\n<h2>동일한 도로 노면 표시 감지</h2>\n<h1>Feature Detection And Matching</h1>\n<h3>1. Bird’s Eyes View</h3>\n<p>카메라 이미지를 Point Cloud 데이터에 Projection 하였을 때 오차가 발생한다. 따라서 조금 더 정확한 이미지로 Feature Detection을 하기 위해 Point Cloud 데이터를 Bird’s Eyes View 이미지로 만들도록 하였다. Point Cloud는 Sparse한 특성을 가지고 있기 때문에 이미지 형태로 변환할 때 비어있는 Pixel에 대해 주변 값들의 평균을 내어 값을 채워주었다.\r\n<img src=\"/blog/4bfe74f228af22076311e9ded646bdbf/0002_bev_blank.gif\" alt=\"\" title=\"빈 Pixel 채우기 BF/AF\"></p>\n<p>사실 도로 위에 촬영 당시 지나가던 자동차와 행인등 여러가지 Noise가 존재하기 때문에 Worm’s Eyes View로 제작하려 하였지만 동일 Pixel값을 가지는 Points중 가장 작은 z좌표를 가지는 포인트를 채택하는 방법은 큰 연산량을 요구하여서 조금 더 최적화가 필요할 것 같다.</p>\n<p>또한 x, y 좌표가 Pixel로(integer) Quantization되기 때문에 어느정도 오차를 감수해야 한다. 이를 보완하는 방법 역시 생각해야 할 것 같다.</p>\n<h3>2. Feature Detection</h3>\n<p>FAST(Features from Accelerated Segment Test) 알고리즘과 BIREF(BInary Robust Independent Elementary Features) 알고리즘을 합친 ORB(Oriented FAST and Rotated BIREF) 알고리즘을 이용하여 Feature Detection을 수행한다.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/7730cf44a25039954b4b913440ce8c69/551ac/0003_feature_detection.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAQAF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAABwpBmP//EABcQAAMBAAAAAAAAAAAAAAAAAAABEBH/2gAIAQEAAQUCFFMP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAEhMRARQaH/2gAIAQEAAT8hb8LDc0Q2RxYf/9oADAMBAAIAAwAAABDAD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQEBAQEBAQAAAAAAAAAAAAERACFBMVH/2gAIAQEAAT8QBKBPDgI4B7gqaEIfp3MRg/twJVe7/9k='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/blog/static/7730cf44a25039954b4b913440ce8c69/a59e9/0003_feature_detection.webp 192w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/0ca9f/0003_feature_detection.webp 384w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/dc9b9/0003_feature_detection.webp 768w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/75745/0003_feature_detection.webp 1125w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/blog/static/7730cf44a25039954b4b913440ce8c69/150b2/0003_feature_detection.jpg 192w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/6985d/0003_feature_detection.jpg 384w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/19de6/0003_feature_detection.jpg 768w,\n/blog/static/7730cf44a25039954b4b913440ce8c69/551ac/0003_feature_detection.jpg 1125w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/blog/static/7730cf44a25039954b4b913440ce8c69/19de6/0003_feature_detection.jpg\"\n            alt=\"0003 feature detection\"\n            title=\"BEV Feature Detection\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Edge가 아닌 부분도 Feature로 검출된 것을 볼 수 있다. Point Cloud의 Sparse한 특성으로 도로 노면에 존재하는 모든</p>\n<h3>3. Feature Matching</h3>\n<p>서로 다른 시간에 동일한 공간을 촬영한 측위 오차가 존재하는 Point Cloud의 BEV 이미지에 ORB를 이용해 추출한 Feature들을 서로 Matching하였다.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/7672c9fbd413e0d685851c907173d981/ad5d3/0004_feature_matching.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAABw4FB/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGRAAAQUAAAAAAAAAAAAAAAAAAAEQITGx/9oACAEBAAE/IcaVo//aAAwDAQACAAMAAAAQcA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEBAAMBAAAAAAAAAAAAAAABEQAhUUH/2gAIAQEAAT8QlL4N9wFJzWQBBM//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/blog/static/7672c9fbd413e0d685851c907173d981/a59e9/0004_feature_matching.webp 192w,\n/blog/static/7672c9fbd413e0d685851c907173d981/0ca9f/0004_feature_matching.webp 384w,\n/blog/static/7672c9fbd413e0d685851c907173d981/dc9b9/0004_feature_matching.webp 768w,\n/blog/static/7672c9fbd413e0d685851c907173d981/e2c2f/0004_feature_matching.webp 1152w,\n/blog/static/7672c9fbd413e0d685851c907173d981/f3efb/0004_feature_matching.webp 1536w,\n/blog/static/7672c9fbd413e0d685851c907173d981/8056f/0004_feature_matching.webp 2250w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/blog/static/7672c9fbd413e0d685851c907173d981/150b2/0004_feature_matching.jpg 192w,\n/blog/static/7672c9fbd413e0d685851c907173d981/6985d/0004_feature_matching.jpg 384w,\n/blog/static/7672c9fbd413e0d685851c907173d981/19de6/0004_feature_matching.jpg 768w,\n/blog/static/7672c9fbd413e0d685851c907173d981/4472f/0004_feature_matching.jpg 1152w,\n/blog/static/7672c9fbd413e0d685851c907173d981/d58d1/0004_feature_matching.jpg 1536w,\n/blog/static/7672c9fbd413e0d685851c907173d981/ad5d3/0004_feature_matching.jpg 2250w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/blog/static/7672c9fbd413e0d685851c907173d981/19de6/0004_feature_matching.jpg\"\n            alt=\"0004 feature matching\"\n            title=\"BEV Feature Matching\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/0eff2397ad4bf17942586a879cbac9fe/ad5d3/0005_feature_matching_valid.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAABw4FB/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGRAAAQUAAAAAAAAAAAAAAAAAAAEQITGx/9oACAEBAAE/IcaVo//aAAwDAQACAAMAAAAQAA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEBAQEBAQAAAAAAAAAAAAABESEAQVH/2gAIAQEAAT8QlL4N+8CkzM6AIJ3/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/blog/static/0eff2397ad4bf17942586a879cbac9fe/a59e9/0005_feature_matching_valid.webp 192w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/0ca9f/0005_feature_matching_valid.webp 384w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/dc9b9/0005_feature_matching_valid.webp 768w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/e2c2f/0005_feature_matching_valid.webp 1152w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/f3efb/0005_feature_matching_valid.webp 1536w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/8056f/0005_feature_matching_valid.webp 2250w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/blog/static/0eff2397ad4bf17942586a879cbac9fe/150b2/0005_feature_matching_valid.jpg 192w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/6985d/0005_feature_matching_valid.jpg 384w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/19de6/0005_feature_matching_valid.jpg 768w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/4472f/0005_feature_matching_valid.jpg 1152w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/d58d1/0005_feature_matching_valid.jpg 1536w,\n/blog/static/0eff2397ad4bf17942586a879cbac9fe/ad5d3/0005_feature_matching_valid.jpg 2250w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/blog/static/0eff2397ad4bf17942586a879cbac9fe/19de6/0005_feature_matching_valid.jpg\"\n            alt=\"0005 feature matching valid\"\n            title=\"Valid Matchings\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Top 10의 Matching중 유효한 Matching은 2개 뿐이었다. 아마 위에서 언급한 이유로 잘못된 Feature들이 검출되기 때문인 것 같다.</p>\n<h2>Citation</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> @INPROCEEDINGS { Argoverse2,\r\n  author = {Benjamin Wilson and William Qi and Tanmay Agarwal and John Lambert and Jagjeet Singh and Siddhesh Khandelwal and Bowen Pan and Ratnesh Kumar and Andrew Hartnett and Jhony Kaesemodel Pontes and Deva Ramanan and Peter Carr and James Hays},\r\n  title = {Argoverse 2: Next Generation Datasets for Self-driving Perception and Forecasting},\r\n  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)},\r\n  year = {2021}\r\n}\r\n@INPROCEEDINGS { TrustButVerify,\r\n  author = {John Lambert and James Hays},\r\n  title = {Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection},\r\n  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)},\r\n  year = {2021}\r\n}</code></pre></div>","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/01_HDCorr.md","frontmatter":{"title":"HDCorr 프로젝트","categories":["HD Corr"],"summary":"HDCorr 프로젝트 개발 일지"}},"allMarkdownRemark":{"nodes":[{"id":"6bb86a4e-7f98-5095-a999-41dd665359b9","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/00_intro.md","frontmatter":{"title":"CLPE 프로젝트 소개"}},{"id":"e77920db-12f8-531d-95fe-8bfceb590992","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/01_dev_diary.md","frontmatter":{"title":"Framework Gatsby"}},{"id":"6c080f02-c683-5af4-b705-f5236707ec06","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/00_intro.md","frontmatter":{"title":"Blog 프로젝트 소개"}},{"id":"34192445-9c6f-55a7-ad47-dd1fa3b854bb","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/01_bert.md","frontmatter":{"title":"BERT"}},{"id":"000f4b64-bb06-53ae-a937-3c73ff4ae75c","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/01_HDCorr.md","frontmatter":{"title":"HDCorr 프로젝트"}},{"id":"447975bf-7c3e-59e7-9275-8979c98cf0c2","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/intro.md","frontmatter":{"title":"Yeti의 블로그"}},{"id":"f2a305d5-e752-5126-90f2-d42ebb7a119a","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/02_implement_quantization.md","frontmatter":{"title":"Implement quantization"}},{"id":"fe1da64d-f7dc-5414-9db2-103eddab4aec","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/02_pointnet.md","frontmatter":{"title":"PointNet"}},{"id":"af88362a-44e5-5b92-80e3-35ccd250e2cc","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/01_pointcloud_manipulator.md","frontmatter":{"title":"Pointcloud Manipulator"}},{"id":"246eb780-220e-5313-9d57-e38f83a9b060","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/99_kitti_dataset.md","frontmatter":{"title":"KITTI Dataset"}},{"id":"f84550e8-e2dd-5561-ae05-e9a5cebcbafe","fileAbsolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/00_calibration.md","frontmatter":{"title":"Calibaration"}}]},"allFile":{"nodes":[{"name":"icon","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"","absolutePath":"C:/Users/yeti/Documents/github/blog/src/images/icon.png"},{"name":"01_dev_diary","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"Blog_project","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/01_dev_diary.md"},{"name":"00_intro","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/00_intro.md"},{"name":"00_intro","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"Blog_project","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/00_intro.md"},{"name":"icon","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/assets/icon.png"},{"name":"01_electron_on_vcxsrv","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_electron_on_vcxsrv.PNG"},{"name":"01_octree","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_octree.png"},{"name":"01_toronto_3d_no_intensity","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_toronto_3d_no_intensity.PNG"},{"name":"01_gatsby_pages","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"Blog_project/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/assets/01_gatsby_pages.PNG"},{"name":"01_sorted_sidebar","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"Blog_project/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/assets/01_sorted_sidebar.PNG"},{"name":"01_bert","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"NLP_Quantization","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/01_bert.md"},{"name":"01_graphql","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"Blog_project/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/Blog_project/assets/01_graphql.PNG"},{"name":"01_bert_models","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"NLP_Quantization/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/assets/01_bert_models.PNG"},{"name":"01_clipping_intensity","birthTime":"2023-05-21 04:36:21","ctime":"2023-05-21 04:36:21","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_clipping_intensity.PNG"},{"name":"01_clipping_exam","birthTime":"2023-05-21 03:26:07","ctime":"2023-05-21 03:26:07","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_clipping_exam.PNG"},{"name":"01_clipping_volume","birthTime":"2023-05-20 04:20:32","ctime":"2023-05-20 04:20:32","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_clipping_volume.PNG"},{"name":"01_KITTI_potree","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_KITTI_potree.PNG"},{"name":"01_toronto_3d_potree_viewer","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_toronto_3d_potree_viewer.PNG"},{"name":"01_slowly_loading_potree","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-20 07:40:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_slowly_loading_potree.gif"},{"name":"01_clipping_volume_intensity","birthTime":"2023-05-21 05:45:02","ctime":"2023-05-21 06:05:18","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_clipping_volume_intensity.PNG"},{"name":"01_HDCorr","birthTime":"2023-09-28 12:41:36","ctime":"2023-10-11 11:24:51","relativeDirectory":"HDCorr","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/01_HDCorr.md"},{"name":"intro","birthTime":"2023-05-20 07:40:29","ctime":"2023-09-28 06:43:36","relativeDirectory":"","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/intro.md"},{"name":"02_implement_quantization","birthTime":"2023-07-15 04:09:28","ctime":"2023-07-16 06:27:58","relativeDirectory":"NLP_Quantization","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/02_implement_quantization.md"},{"name":"02_pointnet","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-23 04:44:04","relativeDirectory":"CLPE","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/02_pointnet.md"},{"name":"01_pointcloud_manipulator","birthTime":"2023-05-20 07:40:29","ctime":"2023-05-27 01:24:21","relativeDirectory":"CLPE","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/01_pointcloud_manipulator.md"},{"name":"99_kitti_dataset","birthTime":"2023-07-05 12:44:44","ctime":"2023-07-05 01:10:49","relativeDirectory":"CLPE","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/99_kitti_dataset.md"},{"name":"00_calibration","birthTime":"2023-05-27 08:15:39","ctime":"2023-06-03 07:35:01","relativeDirectory":"CLPE","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/00_calibration.md"},{"name":"02_stn_2daffine_transformation","birthTime":"2023-05-23 04:43:45","ctime":"2023-05-23 04:43:52","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_stn_2daffine_transformation.PNG"},{"name":"02_stn_example","birthTime":"2023-05-23 03:16:40","ctime":"2023-05-23 03:16:40","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_stn_example.PNG"},{"name":"0003_feature_detection","birthTime":"2023-10-11 11:01:12","ctime":"2023-10-11 11:01:21","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0003_feature_detection.jpg"},{"name":"02_spatial_transformer","birthTime":"2023-05-23 03:19:22","ctime":"2023-05-23 03:20:26","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_spatial_transformer.png"},{"name":"01_cal_is_point_inner_box","birthTime":"2023-05-21 08:04:16","ctime":"2023-05-21 08:04:58","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_cal_is_point_inner_box.jpg"},{"name":"01_get_label_bounding_box","birthTime":"2023-05-22 09:41:10","ctime":"2023-05-22 09:41:47","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/01_get_label_bounding_box.jpg"},{"name":"0004_feature_matching","birthTime":"2023-10-11 11:09:51","ctime":"2023-10-11 11:16:16","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0004_feature_matching.jpg"},{"name":"0005_feature_matching_valid","birthTime":"2023-10-11 11:20:44","ctime":"2023-10-11 11:21:18","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0005_feature_matching_valid.jpg"},{"name":"02_stn_sampler","birthTime":"2023-05-23 03:33:15","ctime":"2023-05-23 03:33:15","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_stn_sampler.PNG"},{"name":"02_stn_grid_generator","birthTime":"2023-05-23 03:26:44","ctime":"2023-05-23 04:41:29","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_stn_grid_generator.PNG"},{"name":"02_road_marking_intensity_distribution","birthTime":"2023-05-22 07:29:29","ctime":"2023-05-22 07:36:03","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_road_marking_intensity_distribution.PNG"},{"name":"02_restore_two_point_with_matrix","birthTime":"2023-05-22 06:00:03","ctime":"2023-05-22 06:00:50","relativeDirectory":"CLPE/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/CLPE/assets/02_restore_two_point_with_matrix.jpg"},{"name":"02_linear_quantization","birthTime":"2023-07-15 05:26:00","ctime":"2023-07-15 05:26:17","relativeDirectory":"NLP_Quantization/assets/02","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/NLP_Quantization/assets/02/02_linear_quantization.jpg"},{"name":"0002_bev_blank","birthTime":"2023-10-11 10:44:50","ctime":"2023-10-11 10:45:30","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0002_bev_blank.gif"},{"name":"0000_nuscene_argoverse","birthTime":"2023-09-28 07:09:59","ctime":"2023-09-28 07:11:52","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0000_nuscene_argoverse.PNG"},{"name":"0001_pcd_pos_error","birthTime":"2023-10-06 04:28:02","ctime":"2023-10-06 04:29:42","relativeDirectory":"HDCorr/assets","absolutePath":"C:/Users/yeti/Documents/github/blog/contents/HDCorr/assets/0001_pcd_pos_error.png"}]}},"pageContext":{"id":"000f4b64-bb06-53ae-a937-3c73ff4ae75c"}},"staticQueryHashes":[],"slicesMap":{}}